---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, 
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# metrica: Prediction performance metrics.

<!-- badges: start -->
[![CRAN status](https://www.r-pkg.org/badges/version/metrica)](https://CRAN.R-project.org/package=metrica)
[![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/grand-total/metrica?color=blue)](https://r-pkg.org/pkg/metrica)
[![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/last-month/metrica?color=blue)](https://r-pkg.org/pkg/metrica) <br/>

[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/adriancorrendo/metrica?branch=master&svg=true)](https://ci.appveyor.com/project/adriancorrendo/metrica)
[![R-CMD-check](https://github.com/adriancorrendo/metrica/workflows/R-CMD-check/badge.svg)](https://github.com/adriancorrendo/metrica/actions)
[![codecov](https://codecov.io/gh/adriancorrendo/metrica/branch/master/graph/badge.svg?token=CfK5NhXzYn)](https://app.codecov.io/gh/adriancorrendo/metrica)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6543296.svg)](https://doi.org/10.5281/zenodo.6543296)

<!-- badges: end -->

A compilation of more than 50 functions designed to evaluate prediction performance of regression (continuous variables) and classification (categorical variables) point-forecast models. Offered scoring rules account for different aspects of the agreement between predicted and observed values. For regression, it includes error metrics (e.g. MAE, RMSE), model efficiencies (e.g. NSE, KGE), indices of agreement (e.g. d, RAC), goodness of fit (e.g. r, R2), concordance correlation (e.g. CCC), error decomposition (e.g. lack of accuracy-precision), and plots the visualize agreement. For classification (binomial and multinomial), it includes functions for confusion matrix, accuracy, precision, recall, specificity, F-score, and Cohen's Kappa. For more details visit the vignettes <https://adriancorrendo.github.io/metrica/>.


The goal of the *metrica* package is to offer users of regression (continuous variables) and classification (categorical variables) point-forecast simulation models (e.g. APSIM, DSSAT, DNDC, Machine Learning) a toolbox with a wide spectrum of goodness of fit, error metrics, indices, and coefficients accounting for different aspects of the agreement between predicted and observed values. Also, _metrica_ some basic visualization functions to assess models performance (e.g. confusion matrix, scatter with regression line; Bland-Altman plot) provided in customizable format (ggplot). 

<img src="man/figures/metrica_logo.png" align="right" height="150" style="float:right; height:150px;"> <br/>

This package contains +50 functions. Two arguments are always required: `observed`(Oi; a.k.a. actual, measured, truth, target) and `predicted` (Pi; a.k.a. simulated, fitted) values. Optional arguments
include `data` that allows to call an existing data frame containing both observed and predicted vectors, and `tidy`, which controls the type of output as a list (tidy = FALSE) or as a data.frame (tidy = TRUE). <br/>

Some functions for regression also require to define the axis `orientation`. For example, 
the slope of the symmetric linear regression describing the bivariate scatter (SMA). 
Current included functions cover both worlds: "regression" (i.e. continuous variables) & 
classification (i.e. nominal or categorical variables). <br/>

Always keep in mind that predicted values should come from out-of-bag samples 
(unseen by training set) to avoid overestimation of prediction performance. <br/>

Check the Documentation at [https://adriancorrendo.github.io/metrica/](https://adriancorrendo.github.io/metrica/) <br/>

**Vignettes** <br/>

[1. Complete list of metrics](https://adriancorrendo.github.io/metrica/articles/available_metrics.html) <br/>

[2. A regression case](https://adriancorrendo.github.io/metrica/articles/vignette1.html) <br/>

[3. A classification case](https://adriancorrendo.github.io/metrica/articles/vignette1.html) <br/>


## 1. Installation

You can install the CRAN version of `metrica` with: <br/>

``` r
install.packages("metrica")
```

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("adriancorrendo/metrica")
```
## 2. Native datasets
The *metrica* package comes with four example datasets from the APSIM
software: <br/>
1. `wheat`. 137 data-points of wheat grain N (grams per squared meter) <br/>
2. `barley`. 69 data-points of barley grain number (x1000 grains per 
squared meter) <br/>
3. `sorghum`. 36 data-points of sorghum grain number (x1000 grains per
squared meter) <br/>
4. `chickpea`. 39 data-points of chickpea aboveground dry mass (kg per
hectare) <br/>

These data correspond to the latest, up-to-date, documentation and
validation of version number 2020.03.27.4956. Data available at:
https://doi.org/10.7910/DVN/EJS4M0. Further details can be found at the official APSIM Next Generation website: https://APSIMnextgeneration.netlify.app/modeldocumentation <br/>


## 3. Example Code

This is a basic example which shows you the core functions of *metrica*:

```{r warning=FALSE, message=FALSE}
library(metrica)
library(dplyr)
library(purrr)
library(tidyr)
library(ggpmisc)

# 1. A. Create a fake dataset
# Set seed for reproducibility
set.seed(1)
# Create a random vector (X) with 100 values
X <- rnorm(n = 100, mean = 0, sd = 10)
# Create a second vector (Y) with 100 values by adding error with respect
# to the first vector (X).
Y <- X + rnorm(n=100, mean = 0, sd = 3)
# Merge vectors in a data frame, rename them as synonyms of observed (measured) and predicted (simulated)
example.data <- data.frame(measured = X, simulated = Y)

# 1. B. Or call native example datasets

example.data <- barley %>%  # or 'wheat', 'sorghum', or 'chickpea'
# 1.b. create columns as synonyms of observed (measured) and predicted (simulated)
                mutate(measured = obs, simulated = pred)  


# 2. Use metrica plot functions
# 2.a. Create scatter plot with PO orientation
barley.scat.plot <- metrica::scatter_plot(data = example.data, obs = measured, pred = simulated,
             orientation = "PO")
barley.scat.plot

# Alternative using vectors instead of dataframe
#metrica::scatter_plot(obs = example.data$obs, pred = example.data$pred)

# 2.b. Create tiles plot with OP orientation
barley.tiles.plot <- 
  metrica::tiles_plot(data = example.data, 
                      obs = measured, pred = simulated,
                      bins = 10, 
                      orientation = "OP",
                      colors = c(low = "pink", high = "steelblue"))

barley.tiles.plot

# 2.c. Create a density plot with OP orientation
barley.density.plot <-
metrica::density_plot(data = example.data, 
                      obs = measured, pred = simulated,
                      n = 5, 
                      orientation = "OP", 
           colors = c(low = "white", high = "steelblue")
           )

barley.density.plot

# 2.d. Create a Bland-Altman plot
barley.ba.plot <- metrica::bland_altman_plot(data = example.data,
                           obs = measured, pred = simulated)

barley.ba.plot

# 3. Get metrics estimates
# 3.a. Single estimates
# 3.a.i. Estimate coefficient of determination (R2)

metrica::R2(data = example.data, obs = measured, pred = simulated)

# 3.a.ii. Estimate root mean squared error (RMSE)
metrica::RMSE(data = example.data, obs = measured, pred = simulated)

# 3.a.iii. Estimate mean bias error (MBE)
metrica::MBE(data = example.data, obs = measured, pred = simulated)

# 3.b. Metrics Summary 
metrics.sum <- metrica::metrics_summary(data = example.data, 
                                        obs = measured, pred = simulated)  
# Print first 15
head(metrics.sum, n = 15)

# Optional wrangling (WIDE)
metrics.sum.wide <- metrics.sum %>%
  tidyr::pivot_wider(tidyr::everything(),
                      names_from = "Metric",
                      values_from = "Score") 

# 4. Test multiple datasets at once
# 4.a. Create nested df with the native examples
nested.examples <- bind_rows(list(wheat = metrica::wheat, 
                                  barley = metrica::barley,
                                  sorghum = metrica::sorghum, 
                                  chickpea = metrica::chickpea), 
                             .id = "id") %>%
  dplyr::group_by(id) %>% tidyr::nest() %>% dplyr::ungroup()

head(nested.examples %>% group_by(id) %>% dplyr::slice_head(n=2))

# 4.b. Run 
multiple.sum <- nested.examples %>% 
  # Store metrics in new.column "performance"
  mutate(performance = map(data, ~metrica::metrics_summary(data=., obs = obs, pred = pred)))

head(multiple.sum)

# 4.c. Non-nested, just with group_by()
non_nested_summary <- nested.examples %>% unnest(cols = "data") %>% 
  group_by(id) %>% 
  summarise(metrics_summary(obs = obs, pred = pred)) %>% 
  dplyr::arrange(Metric)

head(non_nested_summary)

```
## 4. Print metrics in a plot
```{r warning=F, message=F}
df <- metrica::wheat

# B. Create list of selected metrics
selected.metrics <- c("MAE","RMSE", "RRMSE", "R2", "NSE", "KGE", "PLA", "PLP")

plot <- metrica::scatter_plot(data = df, 
                              obs = obs, pred = pred, 
                              print_metrics = TRUE, 
                              metrics_list = selected.metrics,
                              # Customize metrics position
                              position_metrics = c(x = 1 , y = 20),
                              # Customize equation position
                              position_eq = c(x = 7, y = 19.5))

plot
```


## 5. Import simulated data from APSIM Classic (.out) and APSIM NextGen (.db) 
```{r warning=FALSE, message=FALSE}

# Use import_apsim_out for APSIM Classic output
soybean.out <- metrica::import_apsim_out(filepath = "tests/testthat/examples/soybean.out")

head(soybean.out)

# Use import_apsim_db for APSIM NextGeneration output
soybean.db <- metrica::import_apsim_db(filename = "soybean.example.db", folder = "tests/testthat/examples/")

head(soybean.db)

# If observed.data is already as a dataframe, the user may do the match using a simple code like this:
# PO.dataframe <- simulated.data %>% left_join(., observed.data) *by = "col" arg. could be required*

```
  




